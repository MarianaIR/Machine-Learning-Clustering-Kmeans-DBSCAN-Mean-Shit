# ğŸ§© MACHINE LEARNING: CLUSTERING - K-MEANS, DBSCAN Y MEAN SHIFT

[![Python](https://img.shields.io/badge/Python-3670A0?style=flat&logo=python&logoColor=ffdd54)](https://www.python.org/)Â Â 
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)Â Â 
[![Pandas](https://img.shields.io/badge/Pandas-150458?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)Â Â 
[![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=flat&logo=matplotlib&logoColor=white)](https://matplotlib.org/)

Este proyecto es una exploraciÃ³n avanzada del **Aprendizaje No Supervisado**. Se enfoca en comparar y aplicar tres algoritmos de *clustering* de gran relevancia: **K-Means, DBSCAN y Mean Shift**. El objetivo es seleccionar el mÃ©todo mÃ¡s efectivo para la **segmentaciÃ³n de clientes** y el anÃ¡lisis de la **personalidad del cliente** en un contexto de *marketing*.

---

## ğŸ§  Contenido del Proyecto

### 1ï¸âƒ£ AnÃ¡lisis Exploratorio y Dataset
- **Dataset:** Se utiliza el dataset **Customer Personality Analysis** de Kaggle, el cual contiene datos de comportamiento de clientes ideales para una campaÃ±a de *marketing*.
- **Objetivo PrÃ¡ctico:** El anÃ¡lisis busca identificar la manera en que los clientes compran para modificar campaÃ±as y productos ofrecidos.

### 2ï¸âƒ£ Algoritmos de Clustering Implementados
El proyecto ejecuta y compara los resultados de tres metodologÃ­as de *clustering*:

| Algoritmo | Tipo | Principio BÃ¡sico | MÃ©trica Clave de EvaluaciÃ³n |
|:---:|:---:|:---:|:---:|
| **K-Means** | Particional | Agrupa datos en *K* clÃºsteres, minimizando la **distancia euclidiana** entre puntos y su centroide. | **MÃ©todo del Codo** y Coeficiente de Silhouette. |
| **DBSCAN** | Basado en Densidad | Agrupa puntos que estÃ¡n cerca unos de otros (alta densidad), marcando como ruido (*outliers*) los puntos en regiones de baja densidad. | Coeficiente de Silhouette. |
| **Mean Shift** | Basado en Centroides | Mueve los centroides (*mean*) hacia las Ã¡reas de mayor densidad de datos de forma iterativa. | N/A (Generalmente, no requiere selecciÃ³n de *K*). |

### 3ï¸âƒ£ EvaluaciÃ³n y OptimizaciÃ³n
- **Coeficiente de Silhouette:** Se utiliza para evaluar la **calidad de la clusterizaciÃ³n**, midiendo quÃ© tan bien se ha agrupado cada punto. Un valor cercano a +1 indica que el punto estÃ¡ bien agrupado.
- **OptimizaciÃ³n de DBSCAN:** Se demuestra la bÃºsqueda de los hiperparÃ¡metros Ã³ptimos para DBSCAN:
    * **`eps`:** Radio de la vecindad alrededor de un punto.
    * **`min_samples`:** NÃºmero mÃ­nimo de puntos para formar una regiÃ³n densa.
- **Resultado de OptimizaciÃ³n:** El anÃ¡lisis encuentra numÃ©ricamente los mejores valores de `eps` y `min_samples` (ej. 4.0 y 71) que maximizan el Coeficiente de Silhouette.

---

## ğŸ› ï¸ LibrerÃ­as Utilizadas

| LibrerÃ­aÂ  Â  Â  Â | Uso principalÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
|----------------|---------------------------------------------|
| **Scikit-learn**| ImplementaciÃ³n de los algoritmos de *clustering* (`KMeans`, `DBSCAN`, `MeanShift`) y el Coeficiente de Silhouette|
| **Pandas**Â  Â  Â | Carga y manipulaciÃ³n de los datos de clientes|
| **Matplotlib** | VisualizaciÃ³n de los resultados de *clustering* (inferido) |

---

## ğŸ¯ Objetivo del Proyecto
El objetivo es ir mÃ¡s allÃ¡ de la simple aplicaciÃ³n de K-Means, enseÃ±ando al usuario a **seleccionar el algoritmo de *clustering* adecuado** para diferentes estructuras de datos. El proyecto enfatiza la **evaluaciÃ³n rigurosa** de los modelos de *clustering* mediante el Coeficiente de Silhouette y la optimizaciÃ³n de hiperparÃ¡metros.

---

## ğŸ“ˆ Resultados Clave
- Se obtienen y comparan los resultados de segmentaciÃ³n utilizando tres tÃ©cnicas de *clustering* diferentes.
- Se demuestra cÃ³mo utilizar el **Coeficiente de Silhouette** para determinar la calidad de la agrupaciÃ³n en un problema no supervisado.
- Se identifican los **hiperparÃ¡metros Ã³ptimos** para el algoritmo DBSCAN, lo que resulta en una segmentaciÃ³n mÃ¡s precisa y una mejor comprensiÃ³n de la estructura de densidad de los datos.

